import os
import re
from bs4 import BeautifulSoup

def find_api_leaks(directory):
    api_key_patterns = [
        r'[sk|api]_[a-zA-Z0-9]{24}',  # Example: sk_live_...
        r'Bearer [a-zA-Z0-9\-\_]{100,}'  # Example: Bearer token...
    ]
    endpoint_pattern = r'https?://[^\s/$.?#].[^\s]*'

    api_leaks = []

    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith('.html'):
                with open(os.path.join(root, file), 'r', encoding='utf-8') as html_file:
                    soup = BeautifulSoup(html_file, 'html.parser')
                    # Extract URLs from href, src attributes
                    urls = [link.get('href', '') for link in soup.find_all('a')]
                    urls += [script.get('src', '') for script in soup.find_all('script')]
                    # Check for endpoint patterns in URLs
                    for url in urls:
                        if re.match(endpoint_pattern, url):
                            api_leaks.append({'file': os.path.join(root, file), 'endpoint': url})
            elif file.endswith('.js'):
                with open(os.path.join(root, file), 'r', encoding='utf-8') as js_file:
                    content = js_file.read()
                    # Check for API keys and tokens
                    for pattern in api_key_patterns:
                        matches = re.findall(pattern, content)
                        for match in matches:
                            api_leaks.append({'file': os.path.join(root, file), 'api_key': match})

    return api_leaks

# Example usage:
directory_to_scan = '/path/to/your/web/files'
leaks = find_api_leaks(directory_to_scan)
for leak in leaks:
    print(leak)
